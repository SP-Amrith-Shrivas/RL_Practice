{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "TaxiGame.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fDHSNB_OU9Op",
        "outputId": "49b301fc-d135-4f9f-dfd3-56ae734a9ff2"
      },
      "source": [
        "!pip install cmake 'gym[atari]' scipy"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: cmake in /usr/local/lib/python3.6/dist-packages (3.12.0)\n",
            "Requirement already satisfied: gym[atari] in /usr/local/lib/python3.6/dist-packages (0.17.3)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.6/dist-packages (1.4.1)\n",
            "Requirement already satisfied: pyglet<=1.5.0,>=1.4.0 in /usr/local/lib/python3.6/dist-packages (from gym[atari]) (1.5.0)\n",
            "Requirement already satisfied: numpy>=1.10.4 in /usr/local/lib/python3.6/dist-packages (from gym[atari]) (1.19.5)\n",
            "Requirement already satisfied: cloudpickle<1.7.0,>=1.2.0 in /usr/local/lib/python3.6/dist-packages (from gym[atari]) (1.3.0)\n",
            "Requirement already satisfied: opencv-python; extra == \"atari\" in /usr/local/lib/python3.6/dist-packages (from gym[atari]) (4.1.2.30)\n",
            "Requirement already satisfied: atari-py~=0.2.0; extra == \"atari\" in /usr/local/lib/python3.6/dist-packages (from gym[atari]) (0.2.6)\n",
            "Requirement already satisfied: Pillow; extra == \"atari\" in /usr/local/lib/python3.6/dist-packages (from gym[atari]) (7.0.0)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from pyglet<=1.5.0,>=1.4.0->gym[atari]) (0.16.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from atari-py~=0.2.0; extra == \"atari\"->gym[atari]) (1.15.0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hnJqo1u-ZLx-"
      },
      "source": [
        "import gym"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C6xBJ968ZPt6",
        "outputId": "611cd04d-5055-4fb8-d013-db20b6b34fe3"
      },
      "source": [
        "env=gym.make(\"Taxi-v3\").env\r\n",
        "env.render()"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+---------+\n",
            "|\u001b[34;1mR\u001b[0m: | : :\u001b[35mG\u001b[0m|\n",
            "| : | : : |\n",
            "|\u001b[43m \u001b[0m: : : : |\n",
            "| | : | : |\n",
            "|Y| : |B: |\n",
            "+---------+\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-Lnj9pcWZemZ",
        "outputId": "a5b99d58-6a69-4f59-9b0e-89eee772ed02"
      },
      "source": [
        "env.reset()#reset environment to a new, random state\r\n",
        "env.render()\r\n",
        "\r\n",
        "print(\"Action Space {}\".format(env.action_space))\r\n",
        "print(\"State Space {}\".format(env.observation_space))"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+---------+\n",
            "|R: | : :\u001b[34;1m\u001b[43mG\u001b[0m\u001b[0m|\n",
            "| : | : : |\n",
            "| : : : : |\n",
            "| | : | : |\n",
            "|Y| : |\u001b[35mB\u001b[0m: |\n",
            "+---------+\n",
            "\n",
            "Action Space Discrete(6)\n",
            "State Space Discrete(500)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Mxubb8qSbNPR",
        "outputId": "d99b6c06-2134-49d5-d47c-789d19ae5f89"
      },
      "source": [
        "state=env.encode(3,1,2,0) #(taxi row, taxi column, passenger index, destination index)\r\n",
        "print(\"State:\", state)\r\n",
        "\r\n",
        "env.s=state\r\n",
        "env.render()"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "State: 328\n",
            "+---------+\n",
            "|\u001b[35mR\u001b[0m: | : :G|\n",
            "| : | : : |\n",
            "| : : : : |\n",
            "| |\u001b[43m \u001b[0m: | : |\n",
            "|\u001b[34;1mY\u001b[0m| : |B: |\n",
            "+---------+\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eMVqOxDjbhF9",
        "outputId": "231af75e-bc42-4fe9-8389-3e74a61e4248"
      },
      "source": [
        "env.P[328] #{action: [(probability, nextstate, reward, done)]}"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{0: [(1.0, 428, -1, False)],\n",
              " 1: [(1.0, 228, -1, False)],\n",
              " 2: [(1.0, 348, -1, False)],\n",
              " 3: [(1.0, 328, -1, False)],\n",
              " 4: [(1.0, 328, -10, False)],\n",
              " 5: [(1.0, 328, -10, False)]}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GrdWNQUkb2wD"
      },
      "source": [
        "import numpy as np"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S9AG6Jnkb4o-"
      },
      "source": [
        "q_table=np.zeros([env.observation_space.n, env.action_space.n])"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JBCYvFtCcAji",
        "outputId": "b7071c16-291f-4a20-cd40-ef1627234533"
      },
      "source": [
        "%%time\r\n",
        "\"\"\"Training the agent\"\"\"\r\n",
        "\r\n",
        "import random\r\n",
        "from IPython.display import clear_output\r\n",
        "\r\n",
        "#Hyperparameters\r\n",
        "alpha=0.1\r\n",
        "gamma=0.6\r\n",
        "epsilon=0.1\r\n",
        "\r\n",
        "#for plotting metrics\r\n",
        "all_epochs=[]\r\n",
        "all_penalties=[]\r\n",
        "\r\n",
        "for i in range(1,100001):\r\n",
        "  state=env.reset()\r\n",
        "  epochs, penalties, reward=0, 0, 0\r\n",
        "  done=False\r\n",
        "\r\n",
        "  while not done:\r\n",
        "    if random.uniform(0,1) < epsilon:\r\n",
        "      action=env.action_space.sample() #explore action space\r\n",
        "    else:\r\n",
        "      action=np.argmax(q_table[state]) #exploit learned values\r\n",
        "\r\n",
        "    next_state, reward, done, info=env.step(action)\r\n",
        "\r\n",
        "    old_value=q_table[state, action]\r\n",
        "    next_max=np.max(q_table[next_state])\r\n",
        "\r\n",
        "    new_value=(1-alpha) * old_value + alpha * (reward+gamma*next_max)\r\n",
        "    q_table[state,action]=new_value\r\n",
        "\r\n",
        "    if reward ==-10:\r\n",
        "      penalties += 1\r\n",
        "    state=next_state\r\n",
        "    epochs += 1\r\n",
        "\r\n",
        "  if i%100 == 0:\r\n",
        "    clear_output(wait=True)\r\n",
        "    print(f\"Episode: {i}\")\r\n",
        "    print(f\"Penalties: {penalties}\")\r\n",
        "\r\n",
        "  print(\"Training Finsihed!\")"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Episode: 100000\n",
            "Penalties: 0\n",
            "Training Finsihed!\n",
            "CPU times: user 1min 44s, sys: 24.5 s, total: 2min 9s\n",
            "Wall time: 1min 56s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FvG3M5Dxj1rP",
        "outputId": "c8ae795d-cd46-4efc-f169-24223071914e"
      },
      "source": [
        "q_table[328]"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([ -2.40637903,  -2.27325184,  -2.40246382,  -2.3588909 ,\n",
              "       -11.03764391, -10.80570654])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jUkybdqDkhLy",
        "outputId": "de185e98-df22-4a3e-f917-89b0b6ad3e08"
      },
      "source": [
        "#Evaluate agent's performance after QLearning\r\n",
        "total_epochs, total_penalties=0, 0\r\n",
        "episodes=1000\r\n",
        "\r\n",
        "for _ in range(episodes):\r\n",
        "  state=env.reset()\r\n",
        "  epochs, penalties, reward=0, 0, 0\r\n",
        "  done=False\r\n",
        "\r\n",
        "  while not done:\r\n",
        "    action=np.argmax(q_table[state]) #expoitation\r\n",
        "    state, reward, done, info=env.step(action)\r\n",
        "\r\n",
        "    if reward==-10:\r\n",
        "      penalties += 1\r\n",
        "    \r\n",
        "    epochs += 1\r\n",
        "\r\n",
        "  total_penalties += penalties\r\n",
        "  total_epochs += epochs\r\n",
        "\r\n",
        "print(f\"Results after {episodes} episode.\")\r\n",
        "print(f\"Average timesteps per episode: {total_epochs/episodes}\")\r\n",
        "print(f\"Average penalties per episode: {total_penalties/episodes}\")\r\n",
        "print(f\"Total penalties {total_penalties} after 100 episodes.\")"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Results after 1000 episode.\n",
            "Average timesteps per episode: 13.049\n",
            "Average penalties per episode: 0.0\n",
            "Total penalties 0 after 100 episodes.\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}